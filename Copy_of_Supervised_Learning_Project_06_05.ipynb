{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Supervised Learning Project 06/05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R88Ms0MTi0Ma"
      },
      "source": [
        "# Bank Customer Churn Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA6lL1fni0Mb"
      },
      "source": [
        "In this project, we use supervised learning models to identify customers who are likely to churn in the future. Furthermore, we will analyze top factors that influence user retention. [Dataset information](https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO94-bXZi0Md"
      },
      "source": [
        "## Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIvRSRqAi0Md"
      },
      "source": [
        "\n",
        "* [Part 1: Data Exploration](#Part-1:-Data-Exploration)\n",
        "* [Part 2: Feature Preprocessing](#Part-2:-Feature-Preprocessing)\n",
        "* [Part 3: Model Training and Results Evaluation](#Part-3:-Model-Training-and-Result-Evaluation)\n",
        "* [Part 4: Feature Selection](#Part-4:-Feature-Selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUoI2S7Bi6iR"
      },
      "source": [
        "# Part 0: Setup Google Drive Environment\n",
        "check this [link](https://colab.research.google.com/notebooks/io.ipynb) for more info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neechzbWi7rV"
      },
      "source": [
        "# install pydrive to load data\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UScKyL2TjARW"
      },
      "source": [
        "# https://drive.google.com/file/d/1A-sIw_IP5sSiDQzLB0v78J5-zSBCKQyw/view?usp=sharing\n",
        "id = \"1A-sIw_IP5sSiDQzLB0v78J5-zSBCKQyw\"\n",
        "file = drive.CreateFile({'id':id}) \n",
        "file.GetContentFile('bank_churn.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK7A1qhYSDxM"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('bank_churn.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6bG_gAPi0Me"
      },
      "source": [
        "# Part 1: Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bspx2K6fi0Me"
      },
      "source": [
        "### Part 1.1: Understand the Raw Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuTHKjk-i0Mf"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "churn_df = pd.read_csv('bank_churn.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHNZRs2Ti0Mi",
        "scrolled": true
      },
      "source": [
        "churn_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht5YOBdx8NLV"
      },
      "source": [
        "# check data info\n",
        "churn_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZASeB8_089yA"
      },
      "source": [
        "# check the unique values for each column\n",
        "churn_df.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ec5r_Qdi0NL"
      },
      "source": [
        "# Get target variable\n",
        "y = churn_df['Exited']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzCo_GC97rGd"
      },
      "source": [
        "# check the propotion of y = 1\n",
        "# python package: imbalance-learn\n",
        "print(y.sum() / y.shape * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsAbAjhvi0Mx"
      },
      "source": [
        "### Part 1.2:  Understand the features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t1xsBp--_0K"
      },
      "source": [
        "# check missing values\n",
        "churn_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIqBIpOt_COM"
      },
      "source": [
        "# understand Numerical feature\n",
        "# discrete/continuous\n",
        "# 'CreditScore', 'Age', 'Tenure', 'NumberOfProducts'\n",
        "# 'Balance', 'EstimatedSalary'\n",
        "churn_df[['CreditScore', 'Age', 'Tenure', 'NumOfProducts','Balance', 'EstimatedSalary']].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSWC_9arxlfk"
      },
      "source": [
        "# check the feature distribution\n",
        "# pandas.DataFrame.describe()\n",
        "# boxplot, distplot, countplot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6o4PlZbuSYy"
      },
      "source": [
        "# boxplot for numerical feature\n",
        "_,axss = plt.subplots(2,3, figsize=[20,10])\n",
        "sns.boxplot(x='Exited', y ='CreditScore', data=churn_df, ax=axss[0][0])\n",
        "sns.boxplot(x='Exited', y ='Age', data=churn_df, ax=axss[0][1])\n",
        "sns.boxplot(x='Exited', y ='Tenure', data=churn_df, ax=axss[0][2])\n",
        "sns.boxplot(x='Exited', y ='NumOfProducts', data=churn_df, ax=axss[1][0])\n",
        "sns.boxplot(x='Exited', y ='Balance', data=churn_df, ax=axss[1][1])\n",
        "sns.boxplot(x='Exited', y ='EstimatedSalary', data=churn_df, ax=axss[1][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZX_0E6R_E7e"
      },
      "source": [
        "# understand categorical feature\n",
        "# 'Geography', 'Gender'\n",
        "# 'HasCrCard', 'IsActiveMember'\n",
        "_,axss = plt.subplots(2,2, figsize=[20,10])\n",
        "sns.countplot(x='Exited', hue='Geography', data=churn_df, ax=axss[0][0])\n",
        "sns.countplot(x='Exited', hue='Gender', data=churn_df, ax=axss[0][1])\n",
        "sns.countplot(x='Exited', hue='HasCrCard', data=churn_df, ax=axss[1][0])\n",
        "sns.countplot(x='Exited', hue='IsActiveMember', data=churn_df, ax=axss[1][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DKTTdB6i0M2"
      },
      "source": [
        "# correlations between features\n",
        "corr_score = churn_df[['CreditScore', 'Age', 'Tenure', 'NumOfProducts','Balance', 'EstimatedSalary']].corr()\n",
        "\n",
        "# show heapmap of correlations\n",
        "sns.heatmap(corr_score) #it may be small, but it really depends on the industry. For instance, in finance industry, |corr| > 0.2 means 'large'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qfEnNW_i0M5"
      },
      "source": [
        "# check the actual values of correlations\n",
        "corr_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFa4d6t3i0NH"
      },
      "source": [
        "# Part 2: Feature Preprocessing\n",
        "\n",
        "feature encoding, feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMTIEpY7IfPp"
      },
      "source": [
        "Read more for handling [categorical feature](https://github.com/scikit-learn-contrib/categorical-encoding), and there is an awesome package for [encoding](http://contrib.scikit-learn.org/category_encoders/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "MrsjiqTwi0NQ"
      },
      "source": [
        "# ordinal encoding\n",
        "churn_df['Gender'] = churn_df['Gender'] == 'Female'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dx072uC1OJQ"
      },
      "source": [
        "# one hot encoding\n",
        "churn_df = pd.get_dummies(churn_df, columns=['Geography'])\n",
        "# sklearn onehotencoder\n",
        "# is it reasonable to do one hot encoding for geography?\n",
        "# it depends. if the case is geography - covid19, we might give higher value to some states; other case like state area matters, use area"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qMpX0j91IAC"
      },
      "source": [
        "churn_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sfa2fQx2xXa"
      },
      "source": [
        "# Get feature space by dropping useless feature\n",
        "to_drop = ['RowNumber','CustomerId','Surname','Exited']\n",
        "X = churn_df.drop(to_drop, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX0AEIgC3VCG"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3x9ySX_i0Nd"
      },
      "source": [
        "# Part 3: Model Training and Result Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77OjmSl9i0Nf"
      },
      "source": [
        "### Part 3.1: Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uay8Md5li0Nh"
      },
      "source": [
        "# Split data into training and testing\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Reserve 20% for testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, stratify = y, random_state=1)\n",
        "\n",
        "print('training data has ' + str(X_train.shape[0]) + ' observation with ' + str(X_train.shape[1]) + ' features')\n",
        "print('test data has ' + str(X_test.shape[0]) + ' observation with ' + str(X_test.shape[1]) + ' features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBcnZBCby9Nv"
      },
      "source": [
        "why do we want to do standardization?\n",
        "age: 0 - 100\n",
        "height: 1.4 - 2 (m)\n",
        "play basketball: 0, 1\n",
        "use knn: (26 - 30)^2 + (2 - 1.4)^2 play basketball is more correlated with age but not height"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuPhtUkJi0NW"
      },
      "source": [
        "# Scale the data, using standardization\n",
        "# standardization (x-mean)/std\n",
        "# normalization (x-x_min)/(x_max-x_min) ->[0,1]\n",
        "\n",
        "# 1. speed up gradient descent\n",
        "# 2. same scale\n",
        "# 3. algorithm requirments\n",
        "\n",
        "# for example, use training data to train the standardscaler to get mean and std \n",
        "# apply mean and std to both training and testing data.\n",
        "# fit_transform does the training and applying, transform only does applying.\n",
        "# Because we can't use any info from test, and we need to do the same modification\n",
        "# to testing data as well as training data\n",
        "\n",
        "# https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n",
        "# https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "\n",
        "\n",
        "# min-max example: (x-x_min)/(x_max-x_min)\n",
        "# [1,2,3,4,5,6] -> fit(min:1, max:6) (scalar.min = 1, scalar.max = 6) -> transform [(1-1)/(6-1),(2-1)/(6-1)..]\n",
        "# scalar.fit(train) -> min:1, max:100\n",
        "# scalar.transform(apply to x) -> apply min:1, max:100 to X_train\n",
        "# scalar.transform -> apply min:1, max:100 to X_test\n",
        "\n",
        "# scalar.fit -> mean:1, std:100\n",
        "# scalar.transform -> apply mean:1, std:100 to X_train\n",
        "# scalar.transform -> apply mean:1, std:100 to X_test\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)# why not fit on test? b/c we assume we dont have any knowledge of test\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)# why fit on test? b/c we want to have consistency on training and testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4UTtCQTi0Nl"
      },
      "source": [
        "### Part 3.2: Model Training and Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "EAhSxINLi0Nl"
      },
      "source": [
        "#@title build models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Logistic Regression\n",
        "classifier_logistic = LogisticRegression()\n",
        "\n",
        "# K Nearest Neighbors\n",
        "classifier_KNN = KNeighborsClassifier()\n",
        "\n",
        "# Random Forest\n",
        "classifier_RF = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av0IRSoBQ3pe"
      },
      "source": [
        "# Train the model\n",
        "classifier_logistic.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiLuzUDJRBNi"
      },
      "source": [
        "# Prediction of test data\n",
        "classifier_logistic.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjMV04mKRJ30"
      },
      "source": [
        "# Accuracy of test data\n",
        "classifier_logistic.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjUVbn-H3CNF"
      },
      "source": [
        "# 1 - |y_test - y_pred|.sum() / y.dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OCgNSNri0Nn"
      },
      "source": [
        "# Use 5-fold Cross Validation to get the accuracy for different models\n",
        "model_names = ['Logistic Regression','KNN','Random Forest']\n",
        "model_list = [classifier_logistic, classifier_KNN, classifier_RF]\n",
        "count = 0\n",
        "\n",
        "for classifier in model_list:\n",
        "    cv_score = model_selection.cross_val_score(classifier, X_train, y_train, cv=5)\n",
        "    print(cv_score)\n",
        "    print('Model accuracy of ' + model_names[count] + ' is ' + str(cv_score.mean()))\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x1oKkiI3csR"
      },
      "source": [
        "why do we need cv? model selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J-23z78i0Ns"
      },
      "source": [
        "### Part 3.3: Use Grid Search to Find Optimal Hyperparameters\n",
        "alternative: random search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpe9PEAAi0Nt"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# helper function for printing out grid search results \n",
        "def print_grid_search_metrics(gs):\n",
        "    print (\"Best score: \" + str(gs.best_score_))\n",
        "    print (\"Best parameters set:\")\n",
        "    best_parameters = gs.best_params_\n",
        "    for param_name in sorted(best_parameters.keys()):\n",
        "        print(param_name + ':' + str(best_parameters[param_name]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvYo9I5Ti0Nv"
      },
      "source": [
        "#### Part 3.3.1: Find Optimal Hyperparameters - LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOc48syxi0Nx",
        "scrolled": true
      },
      "source": [
        "# Possible hyperparamter options for Logistic Regression Regularization\n",
        "# Penalty is choosed from L1 or L2\n",
        "# C is the reciprocol of lambda value(weight) for L1 and L2\n",
        "\n",
        "# ('l1', 1) ('l1', 5) ('l1', 10) ('l2', 1) ('l2', 5) ('l2', 10)\n",
        "parameters = {\n",
        "    'penalty':('l1', 'l2'), \n",
        "    'C':(0.01, 1, 5, 10) \n",
        "}\n",
        "Grid_LR = GridSearchCV(LogisticRegression(solver='liblinear'),parameters, cv=5)\n",
        "Grid_LR.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN5rU0e-i0N1"
      },
      "source": [
        "# the best hyperparameter combination\n",
        "print_grid_search_metrics(Grid_LR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtkDsXgui0N3"
      },
      "source": [
        "# best model\n",
        "best_LR_model = Grid_LR.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u9YFedOi0N6"
      },
      "source": [
        "#### Part 3.3.2: Find Optimal Hyperparameters: KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o78422XVi0N6"
      },
      "source": [
        "# Possible hyperparamter options for KNN\n",
        "# Choose k\n",
        "parameters = {\n",
        "    'n_neighbors':[1,3,5,7,9] # odd number, b/c even number could give tie result, we need a tie breaker\n",
        "}\n",
        "Grid_KNN = GridSearchCV(KNeighborsClassifier(),parameters, cv=5)\n",
        "Grid_KNN.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydaRZVAIi0N_",
        "scrolled": true
      },
      "source": [
        "# best k\n",
        "print_grid_search_metrics(Grid_KNN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq_qfVpXUJcx"
      },
      "source": [
        "best_KNN_model = Grid_KNN.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKn_oKLSi0OB"
      },
      "source": [
        "#### Part 3.3.3: Find Optimal Hyperparameters: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NniAZIPfi0OC"
      },
      "source": [
        "# Possible hyperparamter options for Random Forest\n",
        "# Choose the number of trees\n",
        "parameters = {\n",
        "    'n_estimators' : [40,60,80]\n",
        "}\n",
        "Grid_RF = GridSearchCV(RandomForestClassifier(),parameters, cv=5)\n",
        "Grid_RF.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScPiI-Bfi0OE",
        "scrolled": true
      },
      "source": [
        "# best number of tress\n",
        "print_grid_search_metrics(Grid_RF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJgfri_Mi0OG"
      },
      "source": [
        "# best random forest\n",
        "best_RF_model = Grid_RF.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjmwORLjqCwZ"
      },
      "source": [
        "best_RF_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxDAOrGIi0OI"
      },
      "source": [
        "####Part 3.4: Model Evaluation - Confusion Matrix (Precision, Recall, Accuracy)\n",
        "\n",
        "class of interest as positive\n",
        "\n",
        "TP: correctly labeled real churn\n",
        "\n",
        "Precision(PPV, positive predictive value): tp / (tp + fp);\n",
        "Total number of true predictive churn divided by the total number of predictive churn;\n",
        "High Precision means low fp, not many return users were predicted as churn users. \n",
        "\n",
        "\n",
        "Recall(sensitivity, hit rate, true positive rate): tp / (tp + fn)\n",
        "Predict most postive or churn user correctly. High recall means low fn, not many churn users were predicted as return users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59mWo2oi6k8a"
      },
      "source": [
        "is it always good to use accuracy?\n",
        "no, since it's an overall eval\n",
        "for imbalanced data, we dont want to see the overall eval. for instance: tumor size vs cancer. many \"no cancer\" and very few \"cancer\", without using model, the accuracy rate is high, but what we really care is \"detect cancer\". we can use presicion or recall to eval what we are interested.\n",
        "Interview question: user growth/revenue growth? which is more important?\n",
        "It depends. For startup, user growth, pay attention to recall of churn; for big company, revenue growth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-tP94iFi0OI"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# calculate accuracy, precision and recall, [[tn, fp],[]]\n",
        "def cal_evaluation(classifier, cm):\n",
        "    tn = cm[0][0]\n",
        "    fp = cm[0][1]\n",
        "    fn = cm[1][0]\n",
        "    tp = cm[1][1]\n",
        "    accuracy  = (tp + tn) / (tp + fp + fn + tn + 0.0)\n",
        "    precision = tp / (tp + fp + 0.0)\n",
        "    recall = tp / (tp + fn + 0.0)\n",
        "    print (classifier)\n",
        "    print (\"Accuracy is: \" + str(accuracy))\n",
        "    print (\"precision is: \" + str(precision))\n",
        "    print (\"recall is: \" + str(recall))\n",
        "    print ()\n",
        "\n",
        "# print out confusion matrices\n",
        "def draw_confusion_matrices(confusion_matricies):\n",
        "    class_names = ['Not','Churn']\n",
        "    for cm in confusion_matrices:\n",
        "        classifier, cm = cm[0], cm[1]\n",
        "        cal_evaluation(classifier, cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpSGaN49i0OL"
      },
      "source": [
        "# Confusion matrix, accuracy, precison and recall for random forest and logistic regression\n",
        "confusion_matrices = [\n",
        "    (\"Random Forest\", confusion_matrix(y_test,best_RF_model.predict(X_test))),\n",
        "    (\"Logistic Regression\", confusion_matrix(y_test,best_LR_model.predict(X_test))),\n",
        "    (\"K nearest neighbor\", confusion_matrix(y_test, best_KNN_model.predict(X_test)))\n",
        "]\n",
        "\n",
        "draw_confusion_matrices(confusion_matrices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvHlyhPBi0OT"
      },
      "source": [
        "### Part 3.4: Model Evaluation - ROC & AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx_3XkgKi0OW"
      },
      "source": [
        "RandomForestClassifier, KNeighborsClassifier and LogisticRegression have predict_prob() function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Os_ZLTvi0OX"
      },
      "source": [
        "#### Part 3.4.1: ROC of RF Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UypvQMVBi0OY"
      },
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn import metrics\n",
        "\n",
        "# Use predict_proba to get the probability results of Random Forest\n",
        "y_pred_rf = best_RF_model.predict_proba(X_test)[:, 1]\n",
        "fpr_rf, tpr_rf, thresh = roc_curve(y_test, y_pred_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyADmHiPTtln"
      },
      "source": [
        "best_RF_model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3PR-PdPi0Ob"
      },
      "source": [
        "# ROC curve of Random Forest result\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve - RF model')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R89IUMYDi0Oe"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# AUC score\n",
        "metrics.auc(fpr_rf,tpr_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1DVqnJVi0Oh"
      },
      "source": [
        "#### Part 3.4.1: ROC of LR Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-q5XJPoi0Oi"
      },
      "source": [
        "# Use predict_proba to get the probability results of Logistic Regression\n",
        "y_pred_lr = best_LR_model.predict_proba(X_test)[:, 1]\n",
        "fpr_lr, tpr_lr, thresh = roc_curve(y_test, y_pred_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc4k8gUYcpNE"
      },
      "source": [
        "best_LR_model.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZSrN-1Mi0Ok"
      },
      "source": [
        "# ROC Curve\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_lr, tpr_lr, label='LR')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve - LR Model')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHAyxishi0On"
      },
      "source": [
        "# AUC score\n",
        "metrics.auc(fpr_lr,tpr_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHHurD8Ii0Oq"
      },
      "source": [
        "# Part 4: Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSx4TPO-i0Or"
      },
      "source": [
        "### Part 4.1:  Logistic Regression Model - Feature Selection Discussion "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtLHUixoi0Ot"
      },
      "source": [
        "The corelated features that we are interested in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbNTNeb7saCy"
      },
      "source": [
        "X_with_corr = X.copy()\n",
        "X_with_corr['SalaryInRMB'] = X['EstimatedSalary'] * 6.91\n",
        "X_with_corr.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQaXOIsUi0Ou",
        "scrolled": true
      },
      "source": [
        "# add L1 regularization to logistic regression\n",
        "# check the coef for feature selection\n",
        "scaler = StandardScaler()\n",
        "X_l1 = scaler.fit_transform(X_with_corr)\n",
        "LRmodel_l1 = LogisticRegression(penalty=\"l1\", C = 0.1, solver='liblinear')\n",
        "LRmodel_l1.fit(X_l1, y)\n",
        "\n",
        "indices = np.argsort(abs(LRmodel_l1.coef_[0]))[::-1]\n",
        "\n",
        "print (\"Logistic Regression (L1) Coefficients\")\n",
        "for ind in range(X_with_corr.shape[1]):\n",
        "  print (\"{0} : {1}\".format(X_with_corr.columns[indices[ind]],round(LRmodel_l1.coef_[0][indices[ind]], 4)))\n",
        "  # L1 shrink coefficient to small"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "majifZZqi0O9"
      },
      "source": [
        "# add L2 regularization to logistic regression\n",
        "# check the coef for feature selection\n",
        "np.random.seed()\n",
        "scaler = StandardScaler()\n",
        "X_l2 = scaler.fit_transform(X_with_corr)\n",
        "LRmodel_l2 = LogisticRegression(penalty=\"l2\", C = 0.1, solver='liblinear', random_state=42)\n",
        "LRmodel_l2.fit(X_l2, y)\n",
        "LRmodel_l2.coef_[0]\n",
        "\n",
        "indices = np.argsort(abs(LRmodel_l2.coef_[0]))[::-1]\n",
        "\n",
        "print (\"Logistic Regression (L2) Coefficients\")\n",
        "for ind in range(X_with_corr.shape[1]):\n",
        "  print (\"{0} : {1}\".format(X_with_corr.columns[indices[ind]],round(LRmodel_l2.coef_[0][indices[ind]], 4)))\n",
        "\n",
        "  # L2 make two highly correlated features' coefficient half/half\n",
        "  # if model can handle multicolinearity, use model, dont manually do it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqs41ydLi0O_"
      },
      "source": [
        "### Part 4.2:  Random Forest Model - Feature Importance Discussion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPxUM2lei0PA"
      },
      "source": [
        "# check feature importance of random forest for feature selection\n",
        "forest = RandomForestClassifier()\n",
        "forest.fit(X, y)\n",
        "\n",
        "importances = forest.feature_importances_\n",
        "\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature importance ranking by Random Forest Model:\")\n",
        "for ind in range(X.shape[1]):\n",
        "  print (\"{0} : {1}\".format(X.columns[indices[ind]],round(importances[indices[ind]], 4)))\n",
        "\n",
        "  # age has large importance, which is consistent with the box-plot insight"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}